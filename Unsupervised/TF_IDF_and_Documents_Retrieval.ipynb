{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF/IDF and Documents Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwQehQ5-uUKK"
      },
      "source": [
        "Lab by Arpan Dasgupta\n",
        "\n",
        "arpan.dasgupta@research.iiit.ac.in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-LttpdjnF9P"
      },
      "source": [
        "## Working with Text 3 : TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwQYQjpHnNgf"
      },
      "source": [
        "The issue with using the Bag of Words method for converting text to vectors is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.\n",
        "\n",
        "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vql_OEkqnXHW"
      },
      "source": [
        "TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.\n",
        "\n",
        "**Term Frequency**: is a scoring of the frequency of the word in the current document. \\\\\n",
        "**Inverse Document Frequency**: is a scoring of how rare the word is across documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkF2PkonoHbN"
      },
      "source": [
        "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsY3AiQooQBm"
      },
      "source": [
        "TF is individual to each document and word, hence we can formulate TF as follows.\n",
        "\n",
        "    tf(t,d) = count of t in d / number of words in d\n",
        "\n",
        "DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n",
        "\n",
        "    df(t) = occurrence of t in documents\n",
        "    idf(t) = log(N/(df + 1)) where N = count of corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h_VBLzZo4ab"
      },
      "source": [
        "Thus the formula for the basic TF-IDF is\n",
        "\n",
        "    tf-idf(t, d) = tf(t, d) * log(N/(df + 1))\n",
        "\n",
        "where\n",
        "\n",
        "    t — term (word) \n",
        "    d — document (set of words)\n",
        "    N — count of corpus\n",
        "    corpus — the total document set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LunNDNJBpSzj"
      },
      "source": [
        "### TF-IDF example code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRxeYq0fkyhI"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QcvJi5mqCFA"
      },
      "source": [
        "documentA = 'the man went out for a walk'\n",
        "documentB = 'the children sat around the fire'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24ilSHxqMIf"
      },
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')\n",
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAzZ6Z2SqVSu"
      },
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "  numOfWordsA[word] += 1\n",
        "\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "  numOfWordsB[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbaZBy1nqmYg"
      },
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dnsLBOjq_2R"
      },
      "source": [
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsOZu28prAod"
      },
      "source": [
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFyKKaxtrHBf"
      },
      "source": [
        "idfs = computeIDF([numOfWordsA, numOfWordsB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xc0m0AgrJds"
      },
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2UgKTMArLqr",
        "outputId": "22b4f0a5-bec3-4b48-aeec-1b8cbe4fc384"
      },
      "source": [
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        for  children       out  the  ...       man       sat      went    around\n",
            "0  0.099021  0.000000  0.099021  0.0  ...  0.099021  0.000000  0.099021  0.000000\n",
            "1  0.000000  0.115525  0.000000  0.0  ...  0.000000  0.115525  0.000000  0.115525\n",
            "\n",
            "[2 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKHToHrTrVTs"
      },
      "source": [
        "The following is the scikit-learn implementation. The values differ slightly because sklearn uses a smoothed version of idf and various other little optimizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2-HPlWrPz8",
        "outputId": "d1f82957-a02c-40c4-a2cd-5631af9d26be"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     around  children      fire      for  ...       sat       the     walk     went\n",
            "0  0.000000  0.000000  0.000000  0.42616  ...  0.000000  0.303216  0.42616  0.42616\n",
            "1  0.407401  0.407401  0.407401  0.00000  ...  0.407401  0.579739  0.00000  0.00000\n",
            "\n",
            "[2 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccdRI_RFr7nX"
      },
      "source": [
        "## Working with Text 4 : Semantic Representation and Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QKgyMHowQtS"
      },
      "source": [
        "A move advanced approach is to compare documents based on how similar their words are. For example, ‘apples’ and ‘oranges’ might be regarded as more similar than ‘apples’ and ‘Jupiter’. Judging word similarity at scale is difficult — one widely used approach is to analyse a large corpus of text and rank words that appear together often as being more similar.\n",
        "\n",
        "This is the basis of the word embedding model GloVe: it maps words into numerical vectors — points in a multi-dimensional space so that words that occur together often are near each other in space. It is an unsupervised learning algorithm, developed at Stanford University."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dqo17NJxljb"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytewX92kwq1I",
        "outputId": "32a85356-f856-407e-d329-74dc41beb63b"
      },
      "source": [
        "!pip install gensim~=3.8\n",
        "!pip install nltk~=3.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim~=3.8 in /usr/local/lib/python3.7/dist-packages (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8) (5.1.0)\n",
            "Requirement already satisfied: nltk~=3.4 in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk~=3.4) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxk16qW-xUZm"
      },
      "source": [
        "Here we do preprocessing in gensim, and also remove any HTML tags that may be present, such as if we have scraped data from the web:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yxLZQheXh3v"
      },
      "source": [
        "We shall aim to find the more similar document to the query string\n",
        "\n",
        "```\n",
        "query_string = 'fruit and vegetables'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPriGsa8raqt"
      },
      "source": [
        "from re import sub\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "query_string = 'fruit and vegetables'\n",
        "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
        "\n",
        "stopwords = ['the', 'and', 'are', 'a']\n",
        "\n",
        "def preprocess(doc):\n",
        "    # Tokenize, clean up input document string\n",
        "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
        "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
        "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
        "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
        "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
        "\n",
        "# Preprocess the documents, including the query string\n",
        "corpus = [preprocess(document) for document in documents]\n",
        "query = preprocess(query_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg_K819Qw7md"
      },
      "source": [
        "Then we create a similarity matrix, that contains the similarity between each pair of words, weighted using the term frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bGMzX9ewdx3",
        "outputId": "5fa6919a-7ed8-4826-c1b2-ea5f08bb4ceb"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import WordEmbeddingSimilarityIndex\n",
        "from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import SoftCosineSimilarity\n",
        "\n",
        "# Load the model: this is a big file, can take a while to download and open\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
        "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
        "\n",
        "# Build the term dictionary, TF-idf model\n",
        "dictionary = Dictionary(corpus+[query])\n",
        "tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "# Create the term similarity matrix.  \n",
        "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUEogtqFxbtB"
      },
      "source": [
        "Finally, we calculate the soft cosine similarity between the query and each of the documents. Unlike the regular cosine similarity (which would return zero for vectors with no overlapping terms), the soft cosine similarity considers word similarity as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vibul1lFwhs3",
        "outputId": "95bf0c67-0efb-4e83-f62b-00e227c2d30a"
      },
      "source": [
        "# Compute Soft Cosine Measure between the query and the documents.\n",
        "query_tf = tfidf[dictionary.doc2bow(query)]\n",
        "\n",
        "index = SoftCosineSimilarity(\n",
        "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
        "            similarity_matrix)\n",
        "\n",
        "doc_similarity_scores = index[query_tf]\n",
        "\n",
        "# Output the sorted similarity scores and documents\n",
        "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
        "for idx in sorted_indexes:\n",
        "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n",
        "\n",
        "# 1    0.688    tomatoes are actually fruit\n",
        "# 0    0.000    cars drive on the road"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \t 0.688 \t tomatoes are actually fruit\n",
            "0 \t 0.000 \t cars drive on the road\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:358: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:358: RuntimeWarning: invalid value encountered in multiply\n",
            "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClnHlaa-xvxg"
      },
      "source": [
        "As we can see GloVe works! Semantic similarity is good for ranking content in order, rather than making specific judgements about whether a document is or is not about a specific topic. There are other ways of using semantic similarity, like Word2Vec.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6BjswFpX6vk"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. What is the requirement for IDF? What happens if we use only TF?\n",
        "2. Why is BoW embeddings not sufficient?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj1te-iaY9V9"
      },
      "source": [
        "## References and Resources\n",
        "\n",
        "1. https://monkeylearn.com/blog/what-is-tf-idf/\n",
        "2. https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n",
        "3. https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32"
      ]
    }
  ]
}